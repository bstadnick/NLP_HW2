{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText Word Vectors and Embedding\n",
    "\n",
    "\n",
    "Load in pre-trained word vectors. Using wiki-news-300d-1M from fasttext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "UNK_IDX = 1\n",
    "PAD_IDX = 0\n",
    "BATCH_SIZE = 1000 # change later\n",
    "\n",
    "# Set device. \n",
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "        \n",
    "print(device)\n",
    "\n",
    "# Loads pretrained embeddings from files, also creates token2id and id2token           \n",
    "def create_embedding(fname, size):\n",
    "    vecs = []\n",
    "    tokens = []\n",
    "    embedding = np.array\n",
    "    with open(fname, 'r') as f:\n",
    "        f.readline() # Skip first line\n",
    "        for i in range(size):\n",
    "            line = f.readline()\n",
    "            if line: \n",
    "                line = line.rstrip().split(' ')\n",
    "                token = line[0]\n",
    "                vec = line[1:]\n",
    "                tokens.append(token)\n",
    "                vecs.append(list(map(float,vec)))    \n",
    "            else:\n",
    "                break # Reached end of file\n",
    "    \n",
    "    # Insert tokens\n",
    "    tokens.insert(0, '<unk>')\n",
    "    tokens.insert(0, '<pad>')\n",
    "    \n",
    "    token2id = {}\n",
    "    id2token = {}\n",
    "    \n",
    "    # Initialize pad weights to all zeroes\n",
    "    pad_weights = torch.zeros([1, 300], dtype=torch.float32)\n",
    "    \n",
    "    unk_weights = torch.zeros([1, 300], dtype=torch.float32)\n",
    "    \n",
    "    # Initialize unk weights from standard normal distribution \n",
    "    unk_weights.normal_() \n",
    "    # Scale weights\n",
    "    unk_weights = unk_weights / 10\n",
    "    \n",
    "    for i in range(len(tokens)):\n",
    "        token2id[tokens[i]] = i\n",
    "        id2token[i] = tokens[i]\n",
    "    \n",
    "    # Create embedding matrix\n",
    "    wiki_embed = torch.cat((pad_weights, unk_weights, torch.FloatTensor(vecs)), dim=0)\n",
    "    return wiki_embed, token2id, id2token\n",
    "                               \n",
    "wiki_embed, token2id, id2token = create_embedding('wiki-news-300d-1M.vec', 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 5898 ; token appeals\n",
      "Token appeals; token id 5898\n"
     ]
    }
   ],
   "source": [
    "# From lab, check token2id and id2token match\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization and Max Sentence Length\n",
    "\n",
    "Tokenize the data sets and compute MAX_SENTENCE_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX_SENTENCE_LENGTH 35\n"
     ]
    }
   ],
   "source": [
    "# From lab\n",
    "def token2index_dataset(token2id, dataset):\n",
    "    categories = {'neutral': 0, 'entailment': 1, 'contradiction': 2}\n",
    "    indices = []\n",
    "    for sent1, sent2, label in dataset:\n",
    "        sent1_idx = [token2id[token] if token in token2id else 1 for token in sent1.split()]\n",
    "        sent2_idx = [token2id[token] if token in token2id else 1 for token in sent2.split()]\n",
    "        indices.append((sent1_idx, sent2_idx, categories[label]))\n",
    "    return indices\n",
    "\n",
    "# Load training set\n",
    "train_df = pd.read_csv('./hw2_data/snli_train.tsv', header=0, sep='\\t')\n",
    "train_dataset = zip(train_df['sentence1'], train_df['sentence2'], train_df['label'])\n",
    "train_dataset_idx = token2index_dataset(token2id, train_dataset)\n",
    "\n",
    "# Load validation set\n",
    "val_df = pd.read_csv('./hw2_data/snli_val.tsv', header=0, sep='\\t')\n",
    "val_dataset = zip(val_df['sentence1'], val_df['sentence2'], val_df['label'])\n",
    "val_dataset_idx = token2index_dataset(token2id, val_dataset)\n",
    "\n",
    "train_sent1, train_sent2, train_labels = zip(*(train_dataset_idx))\n",
    "val_sent1, val_sent2, val_labels = zip(*(val_dataset_idx))\n",
    "\n",
    "# Check to make sure number of sentences equals number of labels\n",
    "assert(len(train_sent1) == len(train_sent2) == len(train_labels))\n",
    "assert(len(val_sent1) == len(val_sent2) == len(val_labels))\n",
    "\n",
    "# Get MAX_SENTENCE_LENGTH\n",
    "def find_max_length(train_df, threshold):\n",
    "    \"\"\"\n",
    "    Helper function that returns the sentence length\n",
    "    greater than (threshold*100)% of the sentences in the\n",
    "    dataframe.\n",
    "    \n",
    "    @param: train_df - dataframe containing the premise and hypothesis sentences\n",
    "    @param: threshold - value between 0 and 1 that specifies what percentage of the\n",
    "    sentences should the returned sentence length be longer than - REWORD\n",
    "    \"\"\"\n",
    "    sent1 = train_df['sentence1'].tolist()\n",
    "    sent2 = train_df['sentence2'].tolist()\n",
    "    sents = sent1 + sent2\n",
    "    sent_lens = list(map(lambda x: len(x.split()), sents))\n",
    "    \n",
    "    # Sentence threshold\n",
    "    num = int(threshold * len(sent_lens))\n",
    "    \n",
    "    c = Counter(sent_lens)\n",
    "\n",
    "    order = c.most_common()\n",
    "    # Sort, so shortest sentences are first\n",
    "    order.sort()\n",
    "    \n",
    "    # Running count of the number of sentences\n",
    "    rc = 0\n",
    "    # Index\n",
    "    i = 0\n",
    "    while rc < num:\n",
    "        rc += order[i][1]\n",
    "        i += 1\n",
    "    \n",
    "    return order[i][0]\n",
    "\n",
    "MAX_SENTENCE_LENGTH = find_max_length(train_df, 0.995)\n",
    "print(\"MAX_SENTENCE_LENGTH\", MAX_SENTENCE_LENGTH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loaders\n",
    "\n",
    "Create the data loaders for the training set and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 1\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Create dataset loader. \n",
    "# Starter code from lab 4.\n",
    "class SNLIDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, snli_instances):\n",
    "        \"\"\"\n",
    "        @param snli_instances: list of tuples containing (sent1, sent2, label)\n",
    "        @param token2id: mapping from token to index\n",
    "        \"\"\"\n",
    "        \n",
    "        self.data_list = [(sent1, sent2) for sent1, sent2, target in snli_instances]\n",
    "        self.target_list = [target for sent1, sent2, target in snli_instances]\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        sent1 = self.data_list[key][0][:MAX_SENTENCE_LENGTH]\n",
    "        sent2 = self.data_list[key][1][:MAX_SENTENCE_LENGTH]\n",
    "        return [(sent1, sent2) , (len(sent1), len(sent2)), self.target_list[key]]\n",
    "        \n",
    "def snli_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so the data all\n",
    "    have the same length\n",
    "    \"\"\"\n",
    "    sent1_list = []\n",
    "    sent2_list = []\n",
    "    label_list = []\n",
    "    sent1_len_list = []\n",
    "    sent2_len_list = []\n",
    "    \n",
    "    for datum in batch:\n",
    "        sent_pairs, len_pairs, label = datum\n",
    "        \n",
    "        label_list.append(label)\n",
    "        sent1_len_list.append(len_pairs[0])\n",
    "        sent2_len_list.append(len_pairs[1])\n",
    "        \n",
    "        # Pad first sentence\n",
    "        sent1_padded = np.pad(np.array(sent_pairs[0]),\n",
    "                             pad_width=((0,MAX_SENTENCE_LENGTH-len_pairs[0])),\n",
    "                             mode=\"constant\", constant_values=0)\n",
    "        sent1_list.append(sent1_padded)\n",
    "        # Pad second sentence\n",
    "        sent2_padded = np.pad(np.array(sent_pairs[1]),\n",
    "                             pad_width=((0,MAX_SENTENCE_LENGTH-len_pairs[1])),\n",
    "                             mode=\"constant\", constant_values=0)\n",
    "        sent2_list.append(sent2_padded)\n",
    "        \n",
    "    # Returns premise sentences, hypothesis sentences, premise sentence lengths, hypothesis sentence lengths, targets\n",
    "    return [torch.from_numpy(np.array(sent1_list)), torch.from_numpy(np.array(sent2_list)),\n",
    "            torch.LongTensor(sent1_len_list), torch.LongTensor(sent2_len_list),\n",
    "            torch.LongTensor(label_list)]\n",
    "        \n",
    "        \n",
    "# Create training dataset loader\n",
    "train_dataset = SNLIDataset(train_dataset_idx)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          collate_fn=snli_collate_func,\n",
    "                                          num_workers=8,\n",
    "                                          shuffle=True)\n",
    "\n",
    "# Create validation dataset loader\n",
    "val_dataset = SNLIDataset(val_dataset_idx)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        collate_fn=snli_collate_func,\n",
    "                                        num_workers=8,\n",
    "                                        shuffle=True)\n",
    "\n",
    "print(len(train_loader), len(val_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RNN Model\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, emb_dim, emb_matrix, hidden_size=10, num_layers=1, num_classes=3, scheme='cat', dropout=0):\n",
    "        \"\"\"\n",
    "        @param: emb_dim - embedding dimension size\n",
    "        @param: emb_matrix - embedding matrix\n",
    "        @param: hidden_size - size of the hidden state\n",
    "        @param: num_layers - number of layers in the RNN\n",
    "        @param: num_classes - number of classes in classification problem\n",
    "        @param: scheme - method for combining premise and hypothesis sentences\n",
    "        @param: dropout - float between 0 and 1 specifying probability of dropout\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.num_layers, self.hidden_size, self.scheme = num_layers, hidden_size, scheme\n",
    "        self.emb = nn.Embedding.from_pretrained(emb_matrix, freeze=True)\n",
    "        self.rnn = nn.GRU(emb_dim, hidden_size, num_layers=self.num_layers, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        # Adjust first linear layer based on combination scheme\n",
    "        if scheme == 'cat':\n",
    "            self.nn1 = nn.Linear(4*hidden_size, hidden_size)\n",
    "        else:\n",
    "            self.nn1 = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout_val = dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Second linear layer\n",
    "        self.nn2 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # Function initializes the activation of recurrent neural net at timestep 0\n",
    "        # Needs to be in format (num_layers, batch_size, hidden_size)\n",
    "        # Multiply by 2 since it is a bidirectional GRU and we have twice as many\n",
    "        # sentences.\n",
    "        hidden = torch.randn(2*self.num_layers, 2*batch_size, self.hidden_size)\n",
    "        hidden = hidden.to(device)\n",
    "        return hidden\n",
    "    \n",
    "    # Function used to create mask to only update unknown vector\n",
    "    def create_mask(self, sent_tensor):\n",
    "        mask = sent_tensor == 1\n",
    "        diags = []\n",
    "        i_diags = []\n",
    "        for row in mask:\n",
    "            diag = torch.diag(row)\n",
    "            i_diag = torch.diag(1 - row)\n",
    "            diags.append(diag.numpy())\n",
    "            i_diags.append(i_diag.numpy())\n",
    "        diags = np.array(diags)\n",
    "        i_diags = np.array(i_diags)\n",
    "        m = torch.from_numpy(diags)\n",
    "        i_m = torch.from_numpy(i_diags)\n",
    "    \n",
    "        return m.float(), i_m.float()\n",
    "    \n",
    "    # Sort sentences in batch to be in descending order by length\n",
    "    # for pack_padded_sequence.\n",
    "    def sort_batch(self, sents, lengths):\n",
    "        \"\"\"\n",
    "        @param: sents - premise and hypothesis sentences concatenated together\n",
    "        @param: lengths - lengths of each sentence concatenated together\n",
    "        \n",
    "        returns the sentences in descending order, the lengths in descending order,\n",
    "        and the order of the indices corresponding to descending order\"\"\"\n",
    "        lengths = lengths.cpu()\n",
    "        sents = sents.cpu()\n",
    "        \n",
    "        ind_dec_order = np.argsort(lengths.numpy())[::-1]\n",
    "        lens_desc = lengths.numpy()[ind_dec_order]\n",
    "        lens_desc = torch.from_numpy(lens_desc)\n",
    "        sents_desc = sents.numpy()[ind_dec_order]\n",
    "        sents_desc = torch.from_numpy(sents_desc)\n",
    "        \n",
    "        sents_desc = sents_desc.to(device)\n",
    "        lens_desc = lens_desc.to(device)\n",
    "        \n",
    "        return sents_desc, lens_desc, ind_dec_order\n",
    "        \n",
    "    # Sort back into original order of the batch\n",
    "    def unsort_batch(self, outputs, ordering):\n",
    "        \"\"\"\n",
    "        @param: outputs - outputs from RNN encoding\n",
    "        @param: ordering - ind_dec_order, the ordering of indices corresponding to sorting\n",
    "        by decreasing length\"\"\"\n",
    "        \n",
    "        original_order = np.argsort(ordering)\n",
    "        outputs_original = outputs[original_order]\n",
    "\n",
    "        # Half by batch size\n",
    "        size = int(outputs.shape[0] / 2)\n",
    "        sent1_outputs, sent2_outputs = outputs_original[:size], outputs_original[size:]\n",
    "        \n",
    "        sent1_outputs, sent2_outputs = sent1_outputs.to(device), sent2_outputs.to(device)\n",
    "        return sent1_outputs, sent2_outputs\n",
    "    \n",
    "    \n",
    "    def forward(self, sent1, sent2, sent1_len, sent2_len):\n",
    "        self.hidden = self.init_hidden(sent1.shape[0])\n",
    "        # Sort batch sentences in descending order\n",
    "        sents = torch.cat((sent1, sent2), 0)\n",
    "        lens = torch.cat((sent1_len, sent2_len), 0)\n",
    "        \n",
    "        sents_desc, lens_desc, ind_dec_order = self.sort_batch(sents, lens)\n",
    "        \n",
    "        # Create mask to update only <UNK>\n",
    "        # m, i_m = self.create_mask(sents_desc)\n",
    "        \n",
    "        # Pass into embedding\n",
    "        res = self.emb(sents_desc)\n",
    "        \n",
    "        # Use mask\n",
    "        # res = torch.matmul(m, res) + torch.matmul(i_m, res.clone().detach())\n",
    "        \n",
    "        # Pass into RNN encoder\n",
    "        res = torch.nn.utils.rnn.pack_padded_sequence(res, lens_desc, batch_first=True)\n",
    "\n",
    "        # Forward prop through RNN\n",
    "        rnn_out, self.hidden = self.rnn(res, self.hidden)\n",
    "        \n",
    "        # Unsort for each direction\n",
    "        sent1_forward_outputs, sent2_forward_outputs = self.unsort_batch(self.hidden[0], ind_dec_order)\n",
    "        sent1_backward_outputs, sent2_backward_outputs = self.unsort_batch(self.hidden[1], ind_dec_order)\n",
    "        \n",
    "        \n",
    "        # Combine sent1 outputs and sent2 outputs\n",
    "        if self.scheme == 'cat':\n",
    "            sent_outputs = torch.cat((sent1_forward_outputs, sent2_forward_outputs, sent1_backward_outputs, sent2_backward_outputs), dim=1)\n",
    "        elif self.scheme == 'add':\n",
    "            sent_outputs = sent1_forward_outputs + sent2_forward_outputs + sent1_backward_outputs + sent2_backward_outputs\n",
    "        elif self.scheme == 'mult':\n",
    "            sent_outputs = sent1_forward_outputs * sent2_forward_outputs * sent1_backward_outputs * sent2_backward_outputs\n",
    "        else:\n",
    "            # Throw error since scheme does not match existing scheme\n",
    "            raise ValueError('Scheme ' + self.scheme + ' not valid.')\n",
    "        \n",
    "        # Pass into first layer\n",
    "        out = self.nn1(sent_outputs)\n",
    "        # Pass into relu\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        # Pass to dropout layer\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Pass into second layer\n",
    "        logits = self.nn2(out)\n",
    "        \n",
    "        return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create CNN Model    \n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, emb_dim, emb_matrix, hidden_size=10, num_layers=2, num_classes=3, scheme='cat', dropout=0):\n",
    "        \"\"\"\n",
    "        @param: emb_dim - embedding dimension size\n",
    "        @param: emb_matrix - embedding matrix\n",
    "        @param: hidden_size - size of the hidden state\n",
    "        @param: num_layers - number of layers in the RNN\n",
    "        @param: num_classes - number of classes in classification problem\n",
    "        @param: scheme - method for combining premise and hypothesis sentences\n",
    "        @param: dropout - float between 0 and 1 specifying probability of dropout\n",
    "        \"\"\"\n",
    "        super(CNN, self).__init__()\n",
    "        self.num_layers, self.hidden_size, self.scheme = num_layers, hidden_size, scheme\n",
    "        self.emb = nn.Embedding.from_pretrained(emb_matrix, freeze=True)\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(emb_dim, hidden_size, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Dropout layer between convolutional layers\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        # Dropout layer between linear layers\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "        # This should change based on how we choose to represent\n",
    "        # the two sentences\n",
    "        if self.scheme == 'cat':\n",
    "            self.nn1 = nn.Linear(2*hidden_size, hidden_size)\n",
    "        else:\n",
    "            self.nn1 = nn.Linear(hidden_size, hidden_size)\n",
    "            \n",
    "        self.nn2 = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "\n",
    "    # Function used to create mask to only update unknown vector\n",
    "    def create_mask(self, sent_tensor):\n",
    "        mask = sent_tensor == 1\n",
    "        diags = []\n",
    "        i_diags = []\n",
    "        for row in mask:\n",
    "            diag = torch.diag(row)\n",
    "            i_diag = torch.diag(1 - row)\n",
    "            diags.append(diag.numpy())\n",
    "            i_diags.append(i_diag.numpy())\n",
    "        diags = np.array(diags)\n",
    "        i_diags = np.array(i_diags)\n",
    "        m = torch.from_numpy(diags)\n",
    "        i_m = torch.from_numpy(i_diags)\n",
    "    \n",
    "        return m.float(), i_m.float()\n",
    "    \n",
    "    \n",
    "    def forward(self, sent1, sent2, sent1_len, sent2_len):\n",
    "        \n",
    "        batch_size = sent1.shape[0]\n",
    "        # Sort batch sentences in descending order\n",
    "        sents = torch.cat((sent1, sent2), 0)\n",
    "        lens = torch.cat((sent1_len, sent2_len), 0)\n",
    " \n",
    "        # m, i_m = self.create_mask(sents)\n",
    "        \n",
    "        # Pass into embedding\n",
    "        res = self.emb(sents)\n",
    "        \n",
    "         \n",
    "        # Use mask\n",
    "        # res = torch.matmul(m, res) + torch.matmul(i_m, res.clone().detach())\n",
    "        \n",
    "\n",
    "\n",
    "        # Pass into CNN encoder\n",
    "\n",
    "        # From lab 4\n",
    "        hidden = self.conv1(res.transpose(1,2)).transpose(1,2)\n",
    "        hidden = F.relu(hidden)\n",
    "        \n",
    "        # Pass to dropout layer\n",
    "        hidden = self.dropout1(hidden)\n",
    "        \n",
    "        hidden = self.conv2(hidden.transpose(1,2)).transpose(1,2)\n",
    "        hidden = F.relu(hidden)\n",
    "        \n",
    "        # Max pool over time\n",
    "        hidden, _ = torch.max(hidden, dim=1)\n",
    "        \n",
    "        # Split sentences\n",
    "        sent1_output, sent2_output = hidden[:batch_size], hidden[batch_size:]\n",
    "        \n",
    "        # Combine premise and hypothesis outputs\n",
    "        if self.scheme == 'cat':\n",
    "            sent_outputs = torch.cat((sent1_output, sent2_output), dim=1)\n",
    "        elif self.scheme == 'add':\n",
    "            sent_outputs = sent1_output + sent2_output\n",
    "        elif self.scheme == 'mult':\n",
    "            sent_outputs = sent1_output * sent2_output\n",
    "        else:\n",
    "            # Throw exception\n",
    "            raise ValueError('Scheme ' + self.scheme + ' not valid.')\n",
    "        \n",
    "        # Pass to first linear layer\n",
    "        out = self.nn1(sent_outputs)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        # Pass to second dropout layer\n",
    "        out = self.dropout2(out)\n",
    "        \n",
    "        # Pass to second lienar layer\n",
    "        logits = self.nn2(out)\n",
    "        \n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions for Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Train model\n",
    "# From lab\n",
    "def test_model(loader, model, criterion):\n",
    "    \"\"\"\n",
    "    Helper function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    @param: model - the model to test\n",
    "    @param: criterion - the cost function used to compute loss\n",
    "    \n",
    "    returns accuracy, average_loss\n",
    "    \"\"\"\n",
    "    cumulative_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model = model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    for sent1, sent2, sent1_len, sent2_len, label in loader:\n",
    "        sent1, sent2, sent1_len, sent2_len, label = sent1.to(device), sent2.to(device), sent1_len.to(device), sent2_len.to(device), label.to(device)\n",
    "        outputs = model(sent1, sent2, sent1_len, sent2_len)\n",
    "        loss = criterion(outputs, label)\n",
    "        cumulative_loss += loss.item()\n",
    "        \n",
    "    \n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "        predicted = probabilities.max(1, keepdim=True)[1]\n",
    "\n",
    "        total += label.size(0)\n",
    "        correct += predicted.eq(label.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total, cumulative_loss)\n",
    "\n",
    "def train_model(train_loader, val_loader, model, criterion, optimizer, num_epochs):\n",
    "    \"\"\"\n",
    "    Helper function that trains a model\n",
    "    @param: train_loader - training dataset loader\n",
    "    @param: val_loader - validation dataset loader\n",
    "    @param: model - model to train\n",
    "    @param: criterion - cost function used to compute loss\n",
    "    @param: optimizer - optimizer used to update parameters\n",
    "    @param: epochs - the number of epochs to train \n",
    "    \"\"\"\n",
    "    x = []\n",
    "    train_accs = []\n",
    "    train_losses = []\n",
    "    val_accs = []\n",
    "    val_losses = []\n",
    "    \n",
    "    model = model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (sent1, sent2, sent1_len, sent2_len, labels) in enumerate(train_loader):\n",
    "            sent1, sent2, sent1_len, sent2_len, labels = sent1.to(device), sent2.to(device), sent1_len.to(device), sent2_len.to(device), labels.to(device)\n",
    "#             model = model.to(device)\n",
    "#             print(\"IN TRAINING, MODEL\", next(model.parameters()).is_cuda)\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(sent1, sent2, sent1_len, sent2_len)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if i > 0:\n",
    "                train_acc, train_loss = test_model(train_loader, model, criterion)\n",
    "                val_acc, val_loss = test_model(val_loader, model, criterion)\n",
    "                \n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Train Acc: {}, Val Acc: {}'.format(\n",
    "                epoch+1, num_epochs, i+1, len(train_loader), train_acc, val_acc))\n",
    "                \n",
    "                train_accs.append(train_acc)\n",
    "                train_losses.append(train_loss)\n",
    "                val_accs.append(val_acc)\n",
    "                val_losses.append(val_loss)\n",
    "                # I think this is right\n",
    "                x.append(epoch + i / len(train_loader))\n",
    "    return x, train_accs, train_losses, val_accs, val_losses\n",
    "\n",
    "\n",
    "# Creates a figure and saves it to path\n",
    "def create_figure(path, title, x, label_value_pairs, x_label, y_label):\n",
    "    \"\"\"\n",
    "    Function to create a plot\n",
    "    @param: path - file path to save figure to\n",
    "    @param: title - plot title\n",
    "    @param: x - list of x-values\n",
    "    @param: label_value_pairs - list of y-values and their labels to plot\n",
    "    @param: x_label - label for x-axis\n",
    "    @param: y_label - label for y-axis\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    ax = plt.subplot(111)\n",
    "    for pair in label_value_pairs:\n",
    "        ax.plot(x, pair[1], label=pair[0])\n",
    "        \n",
    "    # Set title, x_axis, y_axis\n",
    "    fig.suptitle(title)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    \n",
    "    # Add legend\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, labels)\n",
    "    \n",
    "    # Save figure to file path\n",
    "    plt.savefig(path)\n",
    "    \n",
    "    # Close figure\n",
    "    plt.close()\n",
    "    \n",
    "# Saves the state dictionary of a model to the file specified\n",
    "# by path\n",
    "def save_model(model, path):\n",
    "    \"\"\"\n",
    "    @param: model - a PyTorch model whose state dictionary we want to save\n",
    "    @param: path - the file path to save the dictionary to\n",
    "    \"\"\"\n",
    "    model = model.cpu()\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "# Loads the state dictionary from the file specified by path\n",
    "# into the model and returns the model\n",
    "def load_model(model, path):\n",
    "    \"\"\"\n",
    "    @param: model - the PyTorch model we want to load the state dictionary into\n",
    "    @param: path - the file containing the state dictionary\n",
    "    \"\"\"\n",
    "    torch.manual_seed(10)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    return model\n",
    "\n",
    "# Returns the best accuracy and hyperparameter\n",
    "def get_best(results_dict, param, values):\n",
    "    \"\"\"\n",
    "    @param: results_dict - dictionary of results\n",
    "    @param: param - hyperparemeter changed\n",
    "    @param: values - list of values considered for the hyperparameter\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    for val in values:\n",
    "        pairs.append((val, results_dict[param + '_' + str(val) + '_val_acc'][-1]))\n",
    "    order = sorted(pairs, key = lambda pair: pair[1], reverse=True)\n",
    "    return order[0]\n",
    "\n",
    "# Writes the results to a csv file\n",
    "def write_csv(results_dict, filename):\n",
    "    \"\"\"\n",
    "    @param: results_dict - dictionary of results to write \n",
    "    @param: filename - file to save to\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame.from_dict(results_dict)\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "# Create figures for training accuracy, training loss, validation accuracy, and validation loss\n",
    "# from an experiment\n",
    "def create_figures(results_dict, param, values, path, mode):\n",
    "    \"\"\"\n",
    "    @param: results_dict - dictionary of results to create figures from\n",
    "    @param: param - hyperparameter changed\n",
    "    @param: values - list of values considered for the hyperparameter\n",
    "    @param: path - file to save figures to\n",
    "    \"\"\"\n",
    "    titles = {'_train_acc': 'Training Accuracy', '_train_loss': 'Training Loss', \n",
    "             '_val_acc': 'Validation Accuracy', '_val_loss': 'Validation Loss'}\n",
    "    y_labels = {'_train_acc': 'Accuracy', '_train_loss': 'Loss', '_val_acc': 'Accuracy', '_val_loss': 'Loss'}\n",
    "    plots = ['_train_acc', '_train_loss', '_val_acc', '_val_loss']\n",
    "    \n",
    "    names = [param + '_' + str(val) for val in values]\n",
    "    for plot in plots:\n",
    "        label_value_pairs = []\n",
    "        for name in names:\n",
    "            label_value_pairs.append((name + plot, results_dict[name + plot]))\n",
    "        create_figure(path + param + plot + '.png', mode + ' ' + param + ': ' + titles[plot], results_dict['epoch'], label_value_pairs, 'Epoch', y_labels[plot])\n",
    "\n",
    "# Build, train, and evaluate RNN models with different hyperparameter values\n",
    "def run_rnn_experiment(param, values, model_params, train_loader, val_loader):\n",
    "    \"\"\"\n",
    "    @param: param - hyperparameter to change in each model\n",
    "    @param: values - list of values considered for the hyperparameter\n",
    "    @param: model_params - hyperparameter values for the model\n",
    "    @param: train_loader - training dataset data loader\n",
    "    @param: val_loader - validation dataset data loader\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for value in values:\n",
    "        model_params[param] = value\n",
    "        print(model_params)\n",
    "        \n",
    "        model = RNN(300, wiki_embed, **model_params)\n",
    "#         model = model.to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        x, train_acc, train_loss, val_acc, val_loss = train_model(train_loader, val_loader, model, criterion, optimizer, NUM_EPOCHS)\n",
    "        \n",
    "        # Save model\n",
    "        save_model(model, RNN_MODELS_DIR+param+'_'+str(value)+'.pt')\n",
    "        \n",
    "        # Store results\n",
    "        results['epoch'] = x\n",
    "        results[param+'_'+str(value)+'_train_acc'] = train_acc\n",
    "        results[param+'_'+str(value)+'_train_loss'] = train_loss\n",
    "        results[param+'_'+str(value)+'_val_acc'] = val_acc\n",
    "        results[param+'_'+str(value)+'_val_loss'] = val_loss\n",
    "    \n",
    "    # Create figures\n",
    "    create_figures(results, param, values, RNN_FIG_DIR, 'RNN')\n",
    "    \n",
    "    # Write results\n",
    "    write_csv(results, RNN_RESULTS_DIR + param + '_results.csv')\n",
    "    \n",
    "    # Return best performing hyperparameter\n",
    "    return get_best(results, param, values)\n",
    "\n",
    "# Build, train, and evaluate CNN models with different hyperparameter values\n",
    "def run_cnn_experiment(param, values, model_params, train_loader, val_loader):\n",
    "    \"\"\"\n",
    "    @param: param - hyperparameter to change in each model\n",
    "    @param: values - list of values considered for the hyperparameter\n",
    "    @param: model_params - hyperparameter values for the model\n",
    "    @param: train_loader - training dataset data loader\n",
    "    @param: val_loader - validation dataset data loader\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for value in values:\n",
    "        model_params[param] = value\n",
    "        print(model_params)\n",
    "        \n",
    "        model = CNN(300, wiki_embed, **model_params)\n",
    "#         model = model.to(device)\n",
    "#         print(\"CNN MODEL\", next(model.parameters()).is_cuda)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        x, train_acc, train_loss, val_acc, val_loss = train_model(train_loader, val_loader, model, criterion, optimizer, NUM_EPOCHS)\n",
    "        \n",
    "        # Save model\n",
    "        save_model(model, CNN_MODELS_DIR+param+'_'+str(value)+'.pt')\n",
    "        \n",
    "        # Store results\n",
    "        results['epoch'] = x\n",
    "        results[param+'_'+str(value)+'_train_acc'] = train_acc\n",
    "        results[param+'_'+str(value)+'_train_loss'] = train_loss\n",
    "        results[param+'_'+str(value)+'_val_acc'] = val_acc\n",
    "        results[param+'_'+str(value)+'_val_loss'] = val_loss\n",
    "    \n",
    "    # Create figures\n",
    "    create_figures(results, param, values, CNN_FIG_DIR, 'CNN')\n",
    "    \n",
    "    # Write results\n",
    "    write_csv(results, CNN_RESULTS_DIR + param + '_results.csv')\n",
    "    \n",
    "    # Return best performing hyperparameter\n",
    "    return get_best(results, param, values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "Run experiments on both the RNN and CNN architectures. Vary the size of the hidden state and the type of interaction between premise and hypothesis sentences for the RNN. Vary the size of the hidden state, type of interaction between premise and hypothesis sentences, and dropout probability for the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETER TUNING\n",
    "# RNN FIRST\n",
    "# THEN CNN\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(10)\n",
    "\n",
    "RNN_FIG_DIR = 'rnn_figures/'\n",
    "RNN_RESULTS_DIR = 'rnn_results/'\n",
    "RNN_MODELS_DIR = 'rnn_models/'\n",
    "\n",
    "CNN_FIG_DIR = 'cnn_figures/'\n",
    "CNN_RESULTS_DIR = 'cnn_results/'\n",
    "CNN_MODELS_DIR = 'cnn_models/'\n",
    "\n",
    "# CNN_FIG_DIR = None\n",
    "# CNN_RESULTS_DIR = None\n",
    "# CNN_MODELS_DIR = None \n",
    "\n",
    "rnn_model_params = {'hidden_size': 10, 'num_layers': 1, 'num_classes':3, 'scheme': 'cat', 'dropout':0}\n",
    "\n",
    "best = run_rnn_experiment('hidden_size', [50, 100, 250, 500, 1000], rnn_model_params, train_loader, val_loader)\n",
    "\n",
    "print(\"BEST HIDDEN SIZE\", best)\n",
    "rnn_model_params['hidden_size'] = int(best[0])\n",
    "print(\"NEW CONFIGURATION\", rnn_model_params)\n",
    "    \n",
    "best = run_rnn_experiment('scheme', ['mult', 'add', 'cat'], rnn_model_params, train_loader, val_loader)\n",
    "\n",
    "print(\"BEST COMBINATION SCHEME\", best)\n",
    "rnn_model_params['scheme'] = best[0]\n",
    "print(\"NEW CONFIGURATION\", rnn_model_params)\n",
    "\n",
    "############# NOT RUN FOR TIME REASONS ###################\n",
    "# best = run_rnn_experiment('dropout', [0, 0.1, 0.3, 0.5, 0.7, 0.9], rnn_model_params, train_loader, val_loader)\n",
    "\n",
    "# print(\"BEST DROPOUT\", best)\n",
    "# rnn_model_params['dropout'] = float(best[0])\n",
    "# print(\"NEW CONFIGURATION\", rnn_model_params)\n",
    "\n",
    "cnn_model_params = {'hidden_size': 10, 'num_layers': 2, 'num_classes':3, 'scheme': 'cat', 'dropout':0}\n",
    "\n",
    "\n",
    "best = run_cnn_experiment('hidden_size', [50, 100, 250, 500, 1000], cnn_model_params, train_loader, val_loader)\n",
    "\n",
    "print(\"BEST HIDDEN SIZE\", best)\n",
    "cnn_model_params['hidden_size'] = int(best[0])\n",
    "print(\"NEW CONFIGURATION\", cnn_model_params)\n",
    "\n",
    "best = run_cnn_experiment('scheme', ['mult', 'add', 'cat'], cnn_model_params, train_loader, val_loader)\n",
    "\n",
    "print(\"BEST COMBINATION SCHEME\", best)\n",
    "cnn_model_params['scheme'] = best[0]\n",
    "print(\"NEW CONFIGURATION\", cnn_model_params)\n",
    "\n",
    "\n",
    "best = run_cnn_experiment('dropout', [0, 0.1, 0.3, 0.5, 0.7, 0.9], cnn_model_params, train_loader, val_loader)\n",
    "\n",
    "print(\"BEST DROPOUT\", best)\n",
    "cnn_model_params['dropout'] = float(best[0])\n",
    "print(\"NEW CONFIGURATION\", cnn_model_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiNLI Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIDDEN SIZE 1000\n",
      "SCHEME mult\n",
      "DROPOUT 0\n",
      "MNLI PERFORMANCE WITH BEST CNN MODEL:\n",
      "GENRE:  travel VALIDATION ACCURACY:  44.70468431771894 VALIDATION LOSS:  1.0965607166290283\n",
      "GENRE:  fiction VALIDATION ACCURACY:  44.72361809045226 VALIDATION LOSS:  1.1202881336212158\n",
      "GENRE:  slate VALIDATION ACCURACY:  42.21556886227545 VALIDATION LOSS:  1.7067168951034546\n",
      "GENRE:  telephone VALIDATION ACCURACY:  46.069651741293534 VALIDATION LOSS:  1.7242774963378906\n",
      "GENRE:  government VALIDATION ACCURACY:  41.53543307086614 VALIDATION LOSS:  2.248517394065857\n",
      "\n",
      " MNLI PERFORMANCE WITH BEST RNN MODEL:\n",
      "GENRE:  travel VALIDATION ACCURACY:  38.4928716904277 VALIDATION LOSS:  1.2185195684432983\n",
      "GENRE:  fiction VALIDATION ACCURACY:  44.120603015075375 VALIDATION LOSS:  1.1094251871109009\n",
      "GENRE:  slate VALIDATION ACCURACY:  40.21956087824351 VALIDATION LOSS:  2.015760123729706\n",
      "GENRE:  telephone VALIDATION ACCURACY:  38.70646766169154 VALIDATION LOSS:  2.2981666326522827\n",
      "GENRE:  government VALIDATION ACCURACY:  38.77952755905512 VALIDATION LOSS:  2.1687681674957275\n"
     ]
    }
   ],
   "source": [
    "# BEST MODELS PERFORMANCE ON MNLI\n",
    "# Load in mnli_val dataset\n",
    "mnli_df = pd.read_csv('./hw2_data/mnli_val.tsv', header=0, sep='\\t')\n",
    "\n",
    "# Load best CNN model\n",
    "cnn_model = CNN(300, wiki_embed, hidden_size=250, num_layers=2, num_classes=3, scheme='cat', dropout=0.3)\n",
    "cnn_model = load_model(cnn_model, './cnn_models/cnn_models/dropout_0.3.pt')\n",
    "\n",
    "# Load best RNN model\n",
    "rnn_model = RNN(300, wiki_embed, hidden_size=1000, num_layers=1, num_classes=3, scheme='mult', dropout=0)\n",
    "rnn_model = load_model(rnn_model, './rnn_models/rnn_models/scheme_mult.pt')\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_mnli(mnli_dataset, model):\n",
    "    genres = list(set(df['genre'].tolist())) # Get unique genres\n",
    "\n",
    "    for genre in genres:\n",
    "        genre_df = df[df['genre'] == genre]\n",
    "        genre_df = genre_df.drop('genre', axis=1)\n",
    "\n",
    "        # Get indices for dataset\n",
    "        genre_dataset = zip(genre_df['sentence1'], genre_df['sentence2'], genre_df['label'])\n",
    "        genre_indices = token2index_dataset(token2id, genre_dataset)\n",
    "\n",
    "        # Create loader. # Do I need to change the BATCH_SIZE?\n",
    "        genre_dataset = SNLIDataset(genre_indices)\n",
    "        genre_loader = torch.utils.data.DataLoader(dataset=genre_dataset,\n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            collate_fn=snli_collate_func,\n",
    "                                            shuffle=True)\n",
    "        \n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        acc, loss = test_model(genre_loader, model, criterion)\n",
    "        print(\"GENRE: \", genre, \"VALIDATION ACCURACY: \", acc, \"VALIDATION LOSS: \", loss)\n",
    "\n",
    "\n",
    "print(\"MNLI PERFORMANCE WITH BEST CNN MODEL:\")\n",
    "evaluate_mnli(mnli_df, cnn_model)\n",
    "\n",
    "print(\"\\n MNLI PERFORMANCE WITH BEST RNN MODEL:\")\n",
    "evaluate_mnli(mnli_df, rnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correctly and Incorrectly Classified Examples\n",
    "\n",
    "\n",
    "Below is code used to find 3 correctly classified examples and 3 incorrectly classified examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIDDEN SIZE 1000\n",
      "SCHEME mult\n",
      "DROPOUT 0\n",
      "CORRECT INSTANCES\n",
      "PREMISE: Boy jumps in desert while others watch and take photo . \n",
      " HYPOTHESIS: A group of people without cameras are watching a boy . \n",
      " LABEL: 2 PREDICTED: 2 \n",
      "\n",
      "PREMISE: A child rests on her mother lap exhausted from a day of sun and fun at the beach . \n",
      " HYPOTHESIS: The child played outside . \n",
      " LABEL: 1 PREDICTED: 1 \n",
      "\n",
      "PREMISE: An African American wearing a red backpack looks the photographer as he walks past a concrete wall covered in graffiti . \n",
      " HYPOTHESIS: a person wears a backpack \n",
      " LABEL: 1 PREDICTED: 1 \n",
      "\n",
      "\n",
      "\n",
      "INCORRECT INSTANCES\n",
      "PREMISE: A young woman and some friends at a party . \n",
      " HYPOTHESIS: Friends are enjoying their time together . \n",
      " LABEL: 0, PREDICTED: 1 \n",
      "\n",
      "PREMISE: Three Oklahoma Sooners playing football against another team , one of the <unk> with the ball in their possession . \n",
      " HYPOTHESIS: A group of bears are playing a football game . \n",
      " LABEL: 2, PREDICTED: 1 \n",
      "\n",
      "PREMISE: Two grownups are waiting for the bus to arrive on a cold day . \n",
      " HYPOTHESIS: They are waiting for the bus to pick them up . \n",
      " LABEL: 1, PREDICTED: 0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Helper function to convert a tensor of indices back into a sentence\n",
    "# Removes padding\n",
    "def convert_to_words(sent, id2token):\n",
    "    sentence = []\n",
    "    list_of_indices = sent.numpy()[0].tolist()\n",
    "    for idx in list_of_indices:\n",
    "        # Skip padding\n",
    "        if idx != 0: \n",
    "            sentence.append(id2token[idx])\n",
    "    return sentence\n",
    "\n",
    "\n",
    "# MISCLASSIFIED INSTANCES FOR RNN\n",
    "model = RNN(300, wiki_embed, hidden_size=1000, num_layers=1, num_classes=3, scheme='mult', dropout=0)\n",
    "rnn_model = load_model(model, './rnn_models/rnn_models/scheme_mult.pt')\n",
    "\n",
    "# VAL LOADER WITH BATCH SIZE 1\n",
    "val_loader_1 = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                        batch_size=1,\n",
    "                                        collate_fn=snli_collate_func,\n",
    "                                        shuffle=True)\n",
    "\n",
    "# Function to get 3 correctly classified instances and 3 incorrectly classified instances\n",
    "def get_misclassified(model, val_loader, id2token):\n",
    "    \"\"\"\n",
    "    @param: model - model to evaluate\n",
    "    @param: val_loader - validation loader to get instances from\n",
    "    @param: id2token - mapping from indices to tokens\n",
    "    \"\"\"\n",
    "    correct_instances = []\n",
    "    incorrect_instances = []\n",
    "    \n",
    "    model.eval()\n",
    "    for sent1, sent2, sent1_len, sent2_len, label in val_loader:\n",
    "        sent1, sent2, sent1_len, sent2_len, label = sent1.to(device), sent2.to(device), sent1_len.to(device), sent2_len.to(device), label.to(device)\n",
    "        outputs = model(sent1, sent2, sent1_len, sent2_len)\n",
    "        loss = criterion(outputs, label)\n",
    "#         cumulative_loss += loss.item()\n",
    "        \n",
    "        # Convert indices back to tokens\n",
    "        sentence1 = ' '.join(convert_to_words(sent1, id2token))\n",
    "        sentence2 = ' '.join(convert_to_words(sent2, id2token))\n",
    "    \n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "        predicted = probabilities.max(1, keepdim=True)[1]\n",
    "        \n",
    "        correct = predicted.eq(label.view_as(predicted)).sum().item()\n",
    "        if correct == 0 and len(incorrect_instances) < 3:\n",
    "            incorrect_instances.append((sentence1, sentence2, label.item(), predicted.item()))\n",
    "        elif correct == 1 and len(correct_instances) < 3:\n",
    "            correct_instances.append((sentence1, sentence2, label.item(), predicted.item()))\n",
    "            \n",
    "        if len(correct_instances) == 3 and len(incorrect_instances) == 3:\n",
    "            break\n",
    "            \n",
    "    return correct_instances, incorrect_instances\n",
    "\n",
    "correct_instances, incorrect_instances = get_misclassified(rnn_model, val_loader_1, id2token)\n",
    "\n",
    "print(\"CORRECT INSTANCES\")\n",
    "for inst in correct_instances:\n",
    "    print(\"PREMISE: {} \\n HYPOTHESIS: {} \\n LABEL: {} PREDICTED: {} \\n\".format(inst[0], inst[1], inst[2], inst[3]))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"INCORRECT INSTANCES\")\n",
    "for inst in incorrect_instances:\n",
    "    print(\"PREMISE: {} \\n HYPOTHESIS: {} \\n LABEL: {}, PREDICTED: {} \\n\".format(inst[0], inst[1], inst[2], inst[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of Trained Parameters\n",
    "\n",
    "Below is code used to find the number of trained parameters in each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIDDEN SIZE 50\n",
      "SCHEME cat\n",
      "DROPOUT 0\n",
      "HIDDEN SIZE 100\n",
      "SCHEME cat\n",
      "DROPOUT 0\n",
      "HIDDEN SIZE 250\n",
      "SCHEME cat\n",
      "DROPOUT 0\n",
      "HIDDEN SIZE 500\n",
      "SCHEME cat\n",
      "DROPOUT 0\n",
      "HIDDEN SIZE 1000\n",
      "SCHEME cat\n",
      "DROPOUT 0\n",
      "HIDDEN SIZE 1000\n",
      "SCHEME cat\n",
      "DROPOUT 0\n",
      "HIDDEN SIZE 1000\n",
      "SCHEME mult\n",
      "DROPOUT 0\n",
      "HIDDEN SIZE 1000\n",
      "SCHEME add\n",
      "DROPOUT 0\n",
      "RNN HIDDEN SIZE: 50      115803\n",
      "RNN HIDDEN SIZE: 100      281603\n",
      "RNN HIDDEN SIZE: 250      1079003\n",
      "RNN HIDDEN SIZE: 500      3408003\n",
      "RNN HIDDEN SIZE: 1000      11816003\n",
      "RNN SCHEME: CAT      11816003\n",
      "RNN SCHEME: MULT      8816003\n",
      "RNN SCHEME: ADD      8816003\n",
      "CNN HIDDEN SIZE: 50      57803\n",
      "CNN HIDDEN SIZE: 100      140603\n",
      "CNN HIDDEN SIZE: 250      539003\n",
      "CNN HIDDEN SIZE: 500      1703003\n",
      "CNN HIDDEN SIZE: 1000      5906003\n",
      "CNN SCHEME: CAT      539003\n",
      "CNN SCHEME: MULT      476503\n",
      "CNN SCHEME: ADD      476503\n",
      "CNN DROPOUT: 0.0      539003\n",
      "CNN DROPOUT: 0.1      539003\n",
      "CNN DROPOUT: 0.3      539003\n",
      "CNN DROPOUT: 0.5      539003\n",
      "CNN DROPOUT: 0.7      539003\n",
      "CNN DROPOUT: 0.9      539003\n"
     ]
    }
   ],
   "source": [
    "# FIND NUMBER OF PARAMETERS FOR EACH MODEL\n",
    "\n",
    "# Create RNN models\n",
    "rnn_50 = RNN(300, wiki_embed, hidden_size=50, num_layers=1, num_classes=3, scheme='cat', dropout=0)\n",
    "rnn_100 = RNN(300, wiki_embed, hidden_size=100, num_layers=1, num_classes=3, scheme='cat', dropout=0)\n",
    "rnn_250 = RNN(300, wiki_embed, hidden_size=250, num_layers=1, num_classes=3, scheme='cat', dropout=0)\n",
    "rnn_500 = RNN(300, wiki_embed, hidden_size=500, num_layers=1, num_classes=3, scheme='cat', dropout=0)\n",
    "rnn_1000 = RNN(300, wiki_embed, hidden_size=1000, num_layers=1, num_classes=3, scheme='cat', dropout=0)\n",
    "\n",
    "rnn_cat = RNN(300, wiki_embed, hidden_size=1000, num_layers=1, num_classes=3, scheme='cat', dropout=0)\n",
    "rnn_mult = RNN(300, wiki_embed, hidden_size=1000, num_layers=1, num_classes=3, scheme='mult', dropout=0)\n",
    "rnn_add = RNN(300, wiki_embed, hidden_size=1000, num_layers=1, num_classes=3, scheme='add', dropout=0)\n",
    "\n",
    "# Create CNN models\n",
    "cnn_50 = CNN(300, wiki_embed, hidden_size=50, num_layers=1, num_classes=3, scheme='cat', dropout=0)\n",
    "cnn_100 = CNN(300, wiki_embed, hidden_size=100, num_layers=1, num_classes=3, scheme='cat', dropout=0)\n",
    "cnn_250 = CNN(300, wiki_embed, hidden_size=250, num_layers=1, num_classes=3, scheme='cat', dropout=0)\n",
    "cnn_500 = CNN(300, wiki_embed, hidden_size=500, num_layers=1, num_classes=3, scheme='cat', dropout=0)\n",
    "cnn_1000 = CNN(300, wiki_embed, hidden_size=1000, num_layers=1, num_classes=3, scheme='cat', dropout=0)\n",
    "\n",
    "cnn_cat = CNN(300, wiki_embed, hidden_size=250, num_layers=1, num_classes=3, scheme='cat', dropout=0)\n",
    "cnn_mult = CNN(300, wiki_embed, hidden_size=250, num_layers=1, num_classes=3, scheme='mult', dropout=0)\n",
    "cnn_add = CNN(300, wiki_embed, hidden_size=250, num_layers=1, num_classes=3, scheme='add', dropout=0)\n",
    "\n",
    "cnn_dp_0 = CNN(300, wiki_embed, hidden_size=250, num_layers=1, num_classes=3, scheme='cat', dropout=0)\n",
    "cnn_dp_01 = CNN(300, wiki_embed, hidden_size=250, num_layers=1, num_classes=3, scheme='cat', dropout=0.1)\n",
    "cnn_dp_03 = CNN(300, wiki_embed, hidden_size=250, num_layers=1, num_classes=3, scheme='cat', dropout=0.3)\n",
    "cnn_dp_05 = CNN(300, wiki_embed, hidden_size=250, num_layers=1, num_classes=3, scheme='cat', dropout=0.5)\n",
    "cnn_dp_07 = CNN(300, wiki_embed, hidden_size=250, num_layers=1, num_classes=3, scheme='cat', dropout=0.7)\n",
    "cnn_dp_09 = CNN(300, wiki_embed, hidden_size=250, num_layers=1, num_classes=3, scheme='cat', dropout=0.9)\n",
    "\n",
    "names = [\"RNN HIDDEN SIZE: 50\", \"RNN HIDDEN SIZE: 100\", \"RNN HIDDEN SIZE: 250\", \"RNN HIDDEN SIZE: 500\", \"RNN HIDDEN SIZE: 1000\",\n",
    "        \"RNN SCHEME: CAT\", \"RNN SCHEME: MULT\", \"RNN SCHEME: ADD\", \"CNN HIDDEN SIZE: 50\", \"CNN HIDDEN SIZE: 100\",\n",
    "        \"CNN HIDDEN SIZE: 250\", \"CNN HIDDEN SIZE: 500\", \"CNN HIDDEN SIZE: 1000\", \"CNN SCHEME: CAT\", \"CNN SCHEME: MULT\",\n",
    "        \"CNN SCHEME: ADD\", \"CNN DROPOUT: 0.0\", \"CNN DROPOUT: 0.1\", \"CNN DROPOUT: 0.3\", \"CNN DROPOUT: 0.5\",\n",
    "        \"CNN DROPOUT: 0.7\", \"CNN DROPOUT: 0.9\"]\n",
    "\n",
    "models = [rnn_50, rnn_100, rnn_250, rnn_500, rnn_1000, rnn_cat, rnn_mult, rnn_add, cnn_50, cnn_100, cnn_250, cnn_500,\n",
    "         cnn_1000, cnn_cat, cnn_mult, cnn_add, cnn_dp_0, cnn_dp_01, cnn_dp_03, cnn_dp_05, cnn_dp_07, cnn_dp_09]\n",
    "\n",
    "# A function that returns the number of parameters that are\n",
    "# learned in a model.\n",
    "def get_number_of_parameters(model):\n",
    "    total_parameters = 0\n",
    "    # Iterate over parameters\n",
    "    for param in model.parameters():\n",
    "        # Check to see if a learned parameter\n",
    "        if param.requires_grad:\n",
    "            dims = len(param.shape)\n",
    "            num_of_params = 1\n",
    "            # Multiply dimensions to get total number of parameters\n",
    "            for i in range(dims):\n",
    "                num_of_params *= param.shape[i]\n",
    "            total_parameters += num_of_params\n",
    "    return total_parameters\n",
    "\n",
    "for i in range(len(models)):\n",
    "    num_parameters = get_number_of_parameters(models[i])\n",
    "    \n",
    "    print(names[i], \"    \", num_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
